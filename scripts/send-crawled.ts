import WebCrawler from '../lib/crawler';
import MessageSender from '../lib/messageSender';
import { promises as fs } from 'fs';
import path from 'path';
import { LastRunData, ArticlesData } from '../types';

async function sendCrawled(): Promise<void> {
  console.log('ğŸš€ í¬ë¡¤ë§ + ì „ì†¡ í†µí•© í…ŒìŠ¤íŠ¸\n');

  const crawler = new WebCrawler();
  const sender = new MessageSender();

  try {
    // 1. í¬ë¡¤ë§ ì‹¤í–‰
    console.log('=== 1. ë¬¸ì„œ ìˆ˜ì§‘ ì¤‘... ===');
    const articles = await crawler.crawlAllSites();

    if (articles.length === 0) {
      console.log('ğŸ“­ ìˆ˜ì§‘ëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.');
      return;
    }

    // 2. ì´ì „ ì „ì†¡ ê¸°ë¡ í™•ì¸
    const dataDir = path.join(process.cwd(), 'data');
    let lastRun: LastRunData = { lastRunTime: null, sentArticles: [] };

    try {
      const lastRunData = await fs.readFile(
        path.join(dataDir, 'last-run.json'),
        'utf8',
      );
      lastRun = JSON.parse(lastRunData) as LastRunData;
    } catch (error) {
      console.log('ğŸ“ ì´ì „ ì „ì†¡ ê¸°ë¡ì´ ì—†ìŠµë‹ˆë‹¤. ëª¨ë“  ë¬¸ì„œë¥¼ ì „ì†¡í•©ë‹ˆë‹¤.');
    }

    // 3. ìƒˆë¡œìš´ ë¬¸ì„œë§Œ í•„í„°ë§
    const sentUrls = new Set(
      (lastRun.sentArticles || []).map((article) => article.url),
    );
    const newArticles = articles.filter(
      (article) => !sentUrls.has(article.url),
    );

    console.log(`ğŸ“Š ì „ì²´ ë¬¸ì„œ: ${articles.length}ê°œ`);
    console.log(`ğŸ“¨ ìƒˆë¡œìš´ ë¬¸ì„œ: ${newArticles.length}ê°œ`);

    if (newArticles.length === 0) {
      console.log('ğŸ“­ ì „ì†¡í•  ìƒˆë¡œìš´ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.');
      return;
    }

    // 4. ë©”ì‹œì§€ ì „ì†¡
    console.log('\n=== 2. ë©”ì‹œì§€ ì „ì†¡ ì¤‘... ===');
    const messageSent = await sender.sendNotification(newArticles);

    if (messageSent) {
      // 5. ì „ì†¡ ê¸°ë¡ ì—…ë°ì´íŠ¸
      const updatedSentArticles = [
        ...newArticles,
        ...(lastRun.sentArticles || []),
      ].slice(0, 1000); // ìµœëŒ€ 1000ê°œê¹Œì§€ë§Œ ë³´ê´€

      await fs.mkdir(dataDir, { recursive: true });

      const newLastRun: LastRunData = {
        lastRunTime: new Date().toISOString(),
        sentArticles: updatedSentArticles,
        totalSent: newArticles.length,
        jobType: 'manual',
      };

      await fs.writeFile(
        path.join(dataDir, 'last-run.json'),
        JSON.stringify(newLastRun, null, 2),
      );

      // ì „ì²´ ë¬¸ì„œ ë°ì´í„°ë„ ì €ì¥
      const articlesData: ArticlesData = {
        lastUpdated: new Date().toISOString(),
        totalCount: articles.length,
        articles: articles,
      };

      await fs.writeFile(
        path.join(dataDir, 'articles.json'),
        JSON.stringify(articlesData, null, 2),
      );

      console.log(`âœ… ${newArticles.length}ê°œ ë¬¸ì„œ ì „ì†¡ ì™„ë£Œ ë° ê¸°ë¡ ì €ì¥`);

      // í¬ë¡¤ë§ ë©”íŠ¸ë¦­ ì¶œë ¥
      const metrics = crawler.getMetrics();
      console.log('\nğŸ“Š í¬ë¡¤ë§ í†µê³„:');
      console.log(
        `  ì„±ê³µë¥ : ${Math.round((metrics.sitesSucceeded / metrics.sitesProcessed) * 100)}%`,
      );
      console.log(
        `  í•„í„°ë§ íš¨ìœ¨: ${Math.round((metrics.articlesFiltered / metrics.articlesFound) * 100)}%`,
      );
      console.log(
        `  í‰ê·  ì²˜ë¦¬ ì‹œê°„: ${Math.round(metrics.crawlDuration / metrics.sitesProcessed)}ms/ì‚¬ì´íŠ¸`,
      );
    } else {
      console.log('âŒ ë©”ì‹œì§€ ì „ì†¡ ì‹¤íŒ¨');
    }
  } catch (error) {
    console.error(
      'âŒ í†µí•© í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨:',
      error instanceof Error ? error.message : 'Unknown error',
    );
  }
}

// ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰
if (require.main === module) {
  sendCrawled().catch(console.error);
}

export default sendCrawled;
